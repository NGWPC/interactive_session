{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1f8ef7-33c9-4530-a964-1dabbe709377",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TorchGeo: An Introduction to Object Detection Example\n",
    "[https://medium.com/@byeonghyeokyu/torchgeo-an-introduction-to-object-detection-example-b0fd43e89649](https://medium.com/@byeonghyeokyu/torchgeo-an-introduction-to-object-detection-example-b0fd43e89649)\n",
    "- https://doi.org/10.1016/j.isprsjprs.2014.10.002\n",
    "- https://doi.org/10.1109/IGARSS.2019.8898573\n",
    "- https://doi.org/10.3390/rs12060989"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7643618c-e4fb-4ff0-bd70-ec5f7d212019",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b518779-456f-4943-b1f1-843d78442a4a",
   "metadata": {},
   "source": [
    "Install rar and unrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64abc62c-b107-4109-af2f-aef58e80e7f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://www.rarlab.com/rar/rarlinux-x64-5.5.0.tar.gz\n",
    "!tar xzvf rarlinux-x64-5.5.0.tar.gz \n",
    "!sudo cp rar/rar rar/unrar /usr/local/bin/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e3f468-971b-40bc-a8b4-885031cc3e08",
   "metadata": {},
   "source": [
    "Install Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34243d7-4666-484f-9235-e4c15bd36472",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchgeo[all]\n",
    "!pip install gdown\n",
    "!pip install -q -U pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0303b-6e16-4206-8255-c9b52c61d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchgeo\n",
    "from torchgeo.datasets import VHR10\n",
    "from torchgeo.trainers import ObjectDetectionTask\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f8c94-1207-4893-94af-e783eb36f8e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torchgeo.__version__\n",
    "# Need version 0.5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73a10d1-3bdd-4a77-bc27-3468afa38000",
   "metadata": {},
   "source": [
    "## Downloading the VHR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97aa21-f800-4d34-a6fc-40efa4f7869a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, gdown\n",
    "\n",
    "os.makedirs('data/VHR10/', exist_ok=True)\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1--foZ3dV5OCsqXQXT84UeKtrAqc5CkAE'\n",
    "output_path = 'data/VHR10/NWPU VHR-10 dataset.rar'\n",
    "gdown.download(url, output_path, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed147886-523b-448e-b568-95544bc7c154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(sample):\n",
    "    sample[\"image\"] = sample[\"image\"].float() / 255.0\n",
    "    return sample\n",
    "\n",
    "ds = VHR10(\n",
    "    root=\"data/VHR10/\",\n",
    "    split=\"positive\",\n",
    "    transforms=preprocess,\n",
    "    download=True,\n",
    "    checksum=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd65bc-697c-4818-977b-74499af1a62e",
   "metadata": {},
   "source": [
    "## Exploring the VHR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f934897-27e0-41cd-b2cb-829d0a7a8419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"VHR-10 dataset: {len(ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b1844-5c96-49ac-a04c-f84946b2dc50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds[0][\"image\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c226b8-3788-4ffb-a3a5-f0f5ab55dbe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.Size([3, 808, 958])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca18037-925b-49f8-b8a9-34fa23ab8e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = ds[5][\"image\"].permute(1, 2, 0)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca0c0b9-efe6-4add-9d40-a252584b619c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.plot(ds[5])\n",
    "plt.savefig('ground_truth.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55612e0b-2c64-4e1f-9c1c-f77fff7b8d5c",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fab7d1e-dce5-41b7-a75b-d7cbe177ec53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    new_batch = {\n",
    "        \"image\": [item[\"image\"] for item in batch],  # Images\n",
    "        \"boxes\": [item[\"boxes\"] for item in batch],  # Bounding boxes\n",
    "        \"labels\": [item[\"labels\"] for item in batch],  # Labels\n",
    "        \"masks\": [item[\"masks\"] for item in batch],  # Masks\n",
    "    }\n",
    "    return new_batch  # Return the new batch\n",
    "\n",
    "# Data Loader\n",
    "\n",
    "dl = DataLoader(\n",
    "    ds,  # Dataset\n",
    "    batch_size=32,  # Number of data to load at one time\n",
    "    num_workers=2,  # Number of processes to use for data loading\n",
    "    shuffle=True,  # Whether to shuffle the dataset before loading\n",
    "    collate_fn=collate_fn,  # collate_fn function for batch processing\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d50ab-dfc1-4742-8b20-eb1445d5ac94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VariableSizeInputObjectDetectionTask(ObjectDetectionTask):\n",
    "    # Define the training step\n",
    "    def training_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x = batch[\"image\"]  # Image\n",
    "        batch_size = len(x)  # Set batch size (number of images)\n",
    "        y = [\n",
    "            {\"boxes\": batch[\"boxes\"][i], \"labels\": batch[\"labels\"][i]}\n",
    "            for i in range(batch_size)\n",
    "        ] # Extract bounding box and label information for each image\n",
    "        loss_dict = self(x, y)  # Loss\n",
    "        train_loss: Tensor = sum(loss_dict.values())  # Training loss (sum of loss values)\n",
    "        self.log_dict(loss_dict)  # Record loss values\n",
    "        return train_loss  # Return training loss\n",
    "\n",
    "task = VariableSizeInputObjectDetectionTask(\n",
    "    model=\"faster-rcnn\",  # Faster R-CNN model\n",
    "    backbone=\"resnet18\",  # ResNet18 neural network architecture\n",
    "    weights=True,  # Use pretrained weights\n",
    "    in_channels=3,  # Number of channels in the input image (RGB images)\n",
    "    num_classes=11,  # Number of classes to classify (10 + background)\n",
    "    trainable_layers=3,  # Number of trainable layers\n",
    "    lr=1e-3,  # Learning rate\n",
    "    patience=10,  # Set the number of patience iterations for early stopping\n",
    "    freeze_backbone=False,  # Whether to train with the backbone network weights unfrozen\n",
    ")\n",
    "task.monitor = \"loss_classifier\"  # Set the metric to monitor (here, the classifier's loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce136d68-0257-4337-8486-9bf52e11e818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    default_root_dir=\"logs/\",  # Set the default directory\n",
    "    accelerator=\"gpu\",  # Set the type of hardware accelerator for training (using GPU)\n",
    "    devices=[0],  # List of device IDs to use ([0] means the first GPU)\n",
    "    min_epochs=6,  # Set the minimum number of training epochs\n",
    "    max_epochs=100,  # Set the maximum number of training epochs\n",
    "    log_every_n_steps=20,  # Set how often to log after a number of steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e0a2e-af82-4ad7-95f3-e83c52b3e027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Model training\n",
    "trainer.fit(task, train_dataloaders=dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc214b-8e67-49ef-bb8b-20d20d941db1",
   "metadata": {},
   "source": [
    "## Model Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737df6b6-7926-4b7a-b769-26b4beb8792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74914145-55a5-43e0-8bb5-ace7ae407987",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = task.model\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  out = model(batch[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b6df5-9c08-4744-bbcd-e0491914a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample(batch, out, batch_idx):\n",
    "    return {\n",
    "        \"image\": batch[\"image\"][batch_idx],  # Image\n",
    "        \"boxes\": batch[\"boxes\"][batch_idx],  # Actual bounding boxes\n",
    "        \"labels\": batch[\"labels\"][batch_idx],  # Actual labels\n",
    "        \"masks\": batch[\"masks\"][batch_idx],  # Actual masks\n",
    "        \"prediction_labels\": out[batch_idx][\"labels\"],  # Labels predicted by the model\n",
    "        \"prediction_boxes\": out[batch_idx][\"boxes\"],  # Bounding boxes predicted by the model\n",
    "        \"prediction_scores\": out[batch_idx][\"scores\"],  # Confidence scores for each prediction\n",
    "    }\n",
    "\n",
    "batch_idx = 0\n",
    "sample = create_sample(batch, out, batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31842d2d-325b-49ac-8a72-968e9226d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.plot(sample)\n",
    "plt.savefig('inference.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f01cc-dede-42db-8635-7d08d0611c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Sample for Batch Index 3\n",
    "batch_idx = 3\n",
    "sample = create_sample(batch, out, batch_idx)\n",
    "\n",
    "ds.plot(sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66573204-b27f-40d0-86b2-d9f5c94295b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Sample for Batch Index 5\n",
    "batch_idx = 5\n",
    "sample = create_sample(batch, out, batch_idx)\n",
    "\n",
    "ds.plot(sample)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
