<tool id='User.Demo_jupyter_docker_tensorflow_cloud' name='User.Demo_jupyter_docker_tensorflow_cloud'>
  <command interpreter='bash'>main.sh</command>
  <inputs>
    <param name='_pw_service_name' label='Service' type='hidden' value='r-singularity' width='50%_none'></param>
    <section name='service_sec' type='section' title='Jupyter Server' expanded='true'>
        <param name='_pw_path_to_sing' label='Path to singularity container' type='text' value='/contrib/\${USER}/rserver.sif' help='Path to the singularity container in the execution host' width='50%_none'></param>
        <param name="_pw_use_gpus" type="boolean" truevalue="Yes" falsevalue="No" checked="False" label="Use GPUs?" help='Select Yes to run a CUDA application inside a container' width="25%_none" optional='true' float="right"></param>
    </section>
    <section name='host' type='section' title='Service host' expanded='true'>
      <conditional name="partition_or_controller_cond">
            <param name="_pw_partition_or_controller" type="boolean" truevalue="Partition" falsevalue="Controller" checked="True" label="Run service on partition or controller node?" help='Choose to run the service in the controller / login or partition / compute nodes' width="25%_none" optional='true' float="right"></param>
            <when value="Partition">
                <param name='_pw_partition' label='SLURM partition with GPU support' type='text' help='Partition to submit the interactive job. Needs GPUs and CUDA!' value='default' width='50%_none'>
                </param>
                <param name='_pw_walltime' label='Walltime' type='text' help='e.g. 00:10:00 - Amount of time slurm will honor the interactive session.' value='00:20:00' width='50%_none'>
                </param>
                <param name='_pw_numnodes' label='Number of nodes' type='integer' min="1" max="10" help='Number of nodes to request for the interactive session.' value='1' width='50%_none'>
                </param>
                <param name="_pw_exclusive" type="boolean" truevalue="Yes" falsevalue="No" checked="True" label="Exclusive" help='The job allocation can not share nodes with other running jobs' width="25%_none" optional='true' float="right">
                </param>
                <param name='_pw_custom_directives' label='Additional SLURM directives' type='text' help='e.g. ;--mem=1000;--gpus-per-node=1 - Use the semicolon character ; before each parameter. Use no spaces.' value='' width='50%_none'>
                </param>
            </when>
        </conditional>
    </section>
    <section name='advanced_options_other' type='section' title='Advanced options: Miscellaneous' expanded='false'>
        <param name="_pw_stream" type="boolean" truevalue="Yes" falsevalue="No" checked="True" label="Stream slurm output?" help='Select Yes to stream the slurm output from the execution host to the job directory in PW' width="25%_none" optional='true' float="right">
        </param>
    </section>
  </inputs>
  <outputs>
  </outputs>
</tool>
